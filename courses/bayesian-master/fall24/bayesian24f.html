<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Bayesian Modeling</title>
    <meta name="description" content="Bayesian Modeling">
    <meta name="author" content="Yixin Wang">

    <!-- Favicon
    –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="icon" href="images/favicon.ico" type="image/x-icon" />

    <!-- FONT
    –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

    <!-- font-awesome -->
    <link href="../../../css/font-awesome.min.css" rel="stylesheet">

    <!-- Bootstrap -->
    <link href="../../../css/bootstrap.min.css" rel="stylesheet">

    <!-- Style CSS -->
    <link href="../../../css/style.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/normalize.css">
    <link rel="stylesheet" href="../../../css/skeleton.css">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>
<body id="page-top" data-spy="scroll" data-target=".navbar">
<div id="main-wrapper">
<!-- Page Preloader -->
<div id="preloader">
    <div id="status">
        <div class="status-mes"></div>
    </div>
</div>

<div class="columns-block container">



<!-- <div class="four fixedcolumns"> -->
    <!-- <header class="header theiaStickySidebar"> -->


<!-- <div class="nine columns"> -->
<!-- <div class="theiaStickySidebar"> -->
<section class="expertise-wrapper section-wrapper gray-bg">
    <!-- <div class="container-fluid"> -->
        <!-- <div class="row"> -->

<div class="ribbon">

</div>
<div>
<h1>
STATS/DATASCI 551 Fall 2024
</h1>
<h1>
Bayesian Modeling
</h1>
</div>
<hr />

<div class="nav-bar">
<p><a href="#overview">Overview</a> | <a href="#calendar">Course Calendar</a> | <a href="#lectures">Lecture Schedule</a>  | <a href="#project">Final Project</a>
</div>
<hr />

<h2 id="overview"><a name="overview">Overview</a></h2>


<p>This course provides basic concepts and several modern techniques
of Bayesian modeling and computation. They include basic models,
conjugate priors, and posterior computation, as well as techniques
associated with complex models, such as hierarchical models,
spatiotemporal models, and dynamical models. A substantial part of the
course is devoted to computational algorithms based on Markov Chain
Monte Carlo sampling for complex models. If time permits, we will also
introduce advanced topics such as nonparametric Bayes, variational
inference, and Hamiltonian Monte Carlo techniques. Foundational topics
will be discussed when appropriate, although they are not our primary
focus in this course; such topics may include decision theoretic
characterization of Bayesian inference and its relation to frequentist
methods, de Finetti-type theorems and the existence of priors,
objective prior distributions, and Bayesian model selection.</p>


<h3 id="syllabus">Syllabus</h3>

<p>For course policies, course requirements, and grading policies,
please see the syllabus [<a
href="bayesian_syllabus_2024.pdf">link</a>].</p>

<!-- <p>The main requirements of the course are just-in-time teaching (JiTT)
questions (5%), weekly quizzes (24%), weekly homework assignments
(55%), and one final project (16%). Only your top 8 quiz scores and
your top 10 homework scores are counted.</p> -->

<h3 id="piazza">Piazza</h3>
<p>Students should sign up Piazza [<a href="https://piazza.com/umich/fall2024/statdatasci551/home">link</a>] to join course discussions.</p>

<p><b>All communications with the teaching team (the instructor and
the GSIs) should be conducted over Piazza; please do not email. </b>
If you'd like to reach the instructor or the GSIs for private
questions, please post a private note on Piazza that is only visible
to the instructor and the GSIs. See <a
href="https://support.piazza.com/support/solutions/articles/48000616669-post-a-private-note">here</a>
for detailed instructions.  The GSIs and the instructor will be
monitoring piazza, endorsing correct student answers, and answering
questions that remain after a discussion.</p>

<p><b>As a bonus, up to 3 percentage points will be added to your
final course grade based on piazza participation.</b> You will get
(3x/100) bonus percentage points if the number of your total Piazza
contributions is (x * 100)% of the maximum number of contributions
among all students. The number of Piazza contributions will be
determined by Piazza class statistics.</p>

<h3 id="teaching-staff">Teaching Team and Office Hours</h3>
<ul>
<li>Instructor: Yixin Wang </li>
<ul>
<li>Office Hour: Weekly <a
href="https://calendar.google.com/calendar/u/1?cid=Y19vcGlqcWhrZjloM2MzZHBhb2dkcGh1aHU3OEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t">Office
Hours</a>. </li>
</ul></li>

<li>GSI: 
<ul>
<li>Paolo Borello (Office Hour: Monday 9-11AM; Thursday 9-10AM)</li> 
</ul>
In person GSI Office Hours: Angel Hall G219 </br>
<!-- Virtual GSI Office Hours: <a href="https://umich.zoom.us/j/8636631834">https://umich.zoom.us/j/8636631834</a></br> -->
Please refer to course calendar for details.
</li>

<!-- <h3 id="when-where">When and Where</h3> -->


<h2 id="calendar"><a name="calendar">Course Calendar</a></h2>
<ul>
<li>Lecture: Mon/Wed 2:30pm-3:50pm </li>
<li>Location: 140 LORCH </li>
<li>Google Calendar: The Google Calendar below ideally contains all events and deadlines for student's convenience. Please feel free to add this calendar to your Google Calendar by clicking on the plus (+) button on the bottom right corner of the calendar below. Any adhoc changes to the schedule will be visible on the calendar first. </li>

<iframe src="https://calendar.google.com/calendar/embed?height=600&wkst=1&bgcolor=%23ffffff&ctz=America%2FNew_York&showTitle=1&showNav=1&showTabs=1&mode=AGENDA&src=Y2M2MjI2aHRxODhhNzNpZnE3dWtnam84MGtAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&color=%234285F4" style="border-width:0" width="800" height="400" frameborder="0" scrolling="no"></iframe></li>

</ul>

<h2 id="lectures"><a name="lectures">Lecture Schedule</a></h2>

<p>The Schedule is subject to change.</p>

<p>By each date, please read about
the topic at hand; please choose one reading from the list for the
topic.</p>

<!-- Kruschke = Doing Bayesian Data Analysis [<a href="https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf">link</a>]</br> -->
FBSM = A first course in Bayesian statistical methods [<a href="https://link.springer.com/book/10.1007/978-0-387-92407-6">link</a>]</br>
BDA = Bayesian Data Analysis by Gelman [<a href="http://www.stat.columbia.edu/~gelman/book/">link</a>]</br>
PML = Probabilistic Machine Learning: Advanced Topics by Murphy [<a href="https://probml.github.io/pml-book/book2.html">link</a>]</br>
PRML = Pattern Recognition and Machine Learning by Bishop [<a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">link</a>]</br>
<!-- SR = Statistical Rethinking by McElreath [<a href="https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf">link</a>]</br> -->
<p></p>

<table>
<colgroup>
<col width="15%" />
<col width="15%" />
<col width="40%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Date</th>
<th align="left">Topic</th>
<th align="left">Readings</th>
<td align="left"><p></p></td>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><p>Lecture 1</p></td>
<td align="left"><p>08/26</p></td>
<td align="left"><p>Introduction I</p></td>
<td align="left">
<p>BDA ch. 1</br>
<p>FBSM ch. 1</br>
<p>"Bayesian data analysis for newcomers" <a href="https://link.springer.com/article/10.3758/s13423-017-1272-1">(Kruschke and Liddel, 2018)</a></br>
<p><a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/probability_review.pdf">"Review of Probability" (Blei, 2016)</a></br>
<p>"R Basics with Google Colab" <a href="https://colab.research.google.com/drive/1iz6ILnVGt8Qc6UR1l7oTPou4l6WSrw9S?usp=sharing">Notebook</a> <a href="https://www.youtube.com/watch?v=qE_nQPojhhw">Video</a></p></td>
<td align="left"><p></p></td>
</tr>

<tr class="even">
<td align="left"><p>Lecture 2</p></td>
<td align="left"><p>08/28</p></td>
<td align="left"><p>Introduction II</p></td>
<td align="left">
<p>''</br>
<td align="left"><p></p></td>
</tr>



<tr class="odd">
<td align="left"><p>Labor Day</p></td>
<td align="left"><p>09/02</p></td>
<td align="left"><p>------------</p></td>
<td align="left">------------
<!-- <p>"The Basics of Graphical Models" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/graphical-models.pdf">(Blei, 2016)</a></br>
"Statistical Concepts" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/models.pdf">(Blei, 2016)</a></br>
BDA, Sec. 2.1-3</br>
SR, Sec. 2.3-5 and Chap. 3</br>
PRML, Sec. 1.2-3</br>
"Model-based Machine Learning"  <a href="http://www.cs.columbia.edu/~blei/fogm/2022F/readings/Bishop2013.pdf">(Bishop, 2013)</a></p></td> -->
<td align="left"><p></p></td>
<!-- ESL 9.2</p></td> -->
</tr>


<tr class="even">
<td align="left"><p>Lecture 3</p></td>
<td align="left"><p>09/04</p></td>
<td align="left"><p>Interpretation of probabilities and Bayes' formulas I</p></td>
<td align="left">
<p>FBSM ch. 2</br>
<!-- <p>Sec. 4 of "The Exponential Family" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/exponential_families.pdf">(Blei, 2016)</a></br>
PML, Sec 3.1-5</br>
BDA, Sec. 2.4-9</p></td> -->
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 4</p></td>
<td align="left"><p>09/09</p></td>
<td align="left"><p>Interpretation of probabilities and Bayes' formulas II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 5</p></td>
<td align="left"><p>09/11</p></td>
<td align="left"><p>One-parameter models I</p></td>
<td align="left">
<p>BDA ch. 2</br>
<p>FBSM ch. 3</br>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 6</p></td>
<td align="left"><p>09/16</p></td>
<td align="left"><p>One-parameter models II</p></td>
<td align="left">''</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 7</p></td>
<td align="left"><p>09/18</p></td>
<td align="left"><p>Monte Carlo approximation</p></td>
<td align="left"><p>FBSM ch. 4</br>
</p></td>    
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 8</p></td>
<td align="left"><p>09/23</p></td>
<td align="left"><p>The normal model I</p></td>
<td align="left"><p>FBSM ch. 5</br>
</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 9</p></td>
<td align="left"><p>09/25</p></td>
<td align="left"><p>The normal model II</p></td>
<td align="left"><p>''</a></br>
</p></td>
<td align="left"><p></p></td>
</tr>

<tr class="odd">
<td align="left"><p>Lecture 10</p></td>
<td align="left"><p>09/30</p></td>
<td align="left"><p>The normal model III</p></td>
<td align="left"><p>''</br>
</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 11</p></td>
<td align="left"><p>10/02</p></td>
<td align="left"><p>Bayesian Computation and Introduction to Stan I</br>
<td align="left"><p>FBSM ch. 6</br>
<p>BDA ch. 10-12</br>    
</p></td>    
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 12</p></td>
<td align="left"><p>10/07</p></td>
<td align="left"><p>Bayesian Computation and Introduction to Stan II</p></td>
<td align="left"><p>''
<!-- <p>Sec. 1-2 of "Linear regression, Logistic regression, and Generalized Linear Models"<a href="http://www.cs.columbia.edu/~blei/fogm/2014F/lectures/glms.pdf"> (Blei, 2014)</a></br>
BDA, Chap. 14</br>
PML, Sec. 15.1-2</br>
SR, Chap. 4</br></p></td> -->
</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 13</p></td>
<td align="left"><p>10/09</p></td>
<td align="left"><p>Bayesian Computation and Introduction to Stan III</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Fall break</p></td>
<td align="left"><p>10/14</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 14</p></td>
<td align="left"><p>10/16</p></td>
<td align="left"><p>Bayesian Computation and Introduction to Stan IV</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="odd">
<td align="left"><p>Lecture 15</p></td>
<td align="left"><p>10/21</p></td>
<td align="left"><p>Multi-parameter models I</p></td>
<td align="left"><p>FBSM ch. 7</br>
<p>BDA ch. 3</br>    
</p></td>    
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 16</p></td>
<td align="left"><p>10/23</p></td>
<td align="left"></p>Midterm Exam</td>
<td align="left"><p></p></td>    
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 17</p></td>
<td align="left"><p>10/28</p></td>
<td align="left"><p>Multi-parameter models II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p>
</p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 18</p></td>
<td align="left"><p>10/30</p></td>
<td align="left"><p>Group comparisons and hierarchical modeling I</p></td>
<td align="left"><p>FBSM ch. 8</br>
<p>BDA ch. 5</br>    
</p></td>    
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 19</p></td>
<td align="left"><p>11/04</p></td>
<td align="left"><p>Group comparisons and hierarchical modeling II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="odd">
<td align="left"><p>Lecture 20</p></td>
<td align="left"><p>11/06</p></td>
<td align="left"><p>Regression Models I</p></td>
<td align="left"><p>FBSM ch. 9</br>
<p>BDA ch. 14-16</br>    
</p></td>    
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 21</p></td>
<td align="left"><p>11/11</p></td>
<td align="left"><p>Regression Models II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 22</p></td>
<td align="left"><p>11/13</p></td>
<td align="left"><p>Regression Models III</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 23</p></td>
<td align="left"><p>11/18</p></td>
<td align="left"><p>Regression Models IV</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="odd">
<td align="left"><p>Thanksgiving break</p></td>
<td align="left"><p>11/20</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 24</p></td>
<td align="left"><p>11/25</p></td>
<td align="left"><p>Model checking & comparison I</p></td>
<td align="left"><p>BDA ch. 8-9</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 25</p></td>
<td align="left"><p>11/27</p></td>
<td align="left"><p>Model checking & comparison II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>
<!-- <tr class="even"> -->
<!-- <td align="left"><p>Lecture 25</p></td>
<td align="left"><p>11/29</p></td>
<td align="left"><p>Deep Generative Models and Black Box Variational Inference I</p></td>
<td align="left"><p>"Variational Autoencoders"<a href="https://deepgenerativemodels.github.io/notes/vae/"> (Grover, 2018)</a></br>
[Video Tutorial] "Variational Inference: Foundations and Innovations" (Blei, 2019) <a href="https://www.youtube.com/watch?v=Wd7R_YX4PcQ">(Part 2) </a> [<a href="http://www.cs.columbia.edu/~blei/talks/Blei_VI_tutorial.pdf">slides</a>]</br>
"An Introduction to Variational Autoencoders" Chap. 1-2 <a href="https://arxiv.org/pdf/1906.02691.pdf">(Kingma and Welling, 2019)</a></br>
PML, Sec. 10.3, 21.2</br>
</p></td>
<td align="left"><p></p></td>
</tr>
 -->

<tr class="odd">
<td align="left"><p>Lecture 26</p></td>
<td align="left"><p>12/02</p></td>
<td align="left"><p>Finite mixture models I</p></td>
<td align="left"><p>BDA, Chap. 22</br>
<p>"Bayesian Mixture Models and the Gibbs Sampler"<a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/gibbs.pdf"> (Blei, 2016)</a></br></a></p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 27</p></td>
<td align="left"><p>12/04</p></td>
<td align="left"><p>Finite mixture models II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 28</p></td>
<td align="left"><p>12/09</p></td>
<td align="left"><p>Bayesian decision theory; Summary (and wiggle room)</p></td>
<td align="left"><p>BDA ch. 9</p></td>
<td align="left"><p></p></td>
</tr>



</tbody>
</table>
<hr />


<!-- 

<h2 id="project"><a name="project">Final Project</a></h2>

<p>The final project is an individual project. For requirements of the
final project, please see the <a href="bayesian_project.pdf">final
project guidelines</a>. The LaTeX template for the project report is
<a href="template.zip">here</a>. </p> -->

<!-- <h2 id="tutorials"><a name="tutorials">Tutorials</a></h2>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Date</th>
<th align="left">Topic</th>
<th align="left">Materials</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Tutorial 1</td>
<td align="left">01.10</td>
<td align="left">Probability review</td>
<td align="left">[<a href="tut/tut01.pdf">slides</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 2</td>
<td align="left">01.17</td>
<td align="left">Linear algebra review</td>
<td align="left">[<a href="tut/tut02.pdf">slides</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut02.ipynb">demo</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut02_worksheet.ipynb">exercise</a>]</td>
</tr>
<tr class="odd">
<td align="left">Tutorial 3</td>
<td align="left">01.24</td>
<td align="left">Gradient descent</td>
<td align="left">[<a href="tut/tut03.pdf">slides</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut03.ipynb">demo</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut03_worksheet.ipynb">exercise</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 4</td>
<td align="left">01.31</td>
<td align="left">Linear algebra review</td>
<td align="left">[<a href="tut/tut04.pdf">slides</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut04.ipynb">demo</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut04_worksheet.ipynb">exercise</a>]</td>
</tr>
<tr class="odd">
<td align="left">Tutorial 5</td>
<td align="left">02.07</td>
<td align="left">Midterm review</td>
<td align="left">[<a href="tut/tut05.pdf">slides</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 6</td>
<td align="left">02.28</td>
<td align="left">MCMC</td>
<td align="left">[<a href="tut/tut06.pdf">slides</a>]</td>
</tr>
<tr class="odd">
<td align="left">Tutorial 7</td>
<td align="left">03.14</td>
<td align="left">Multivariate Gaussian</td>
<td align="left">[<a href="tut/tut07.pptx">slides</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 8</td>
<td align="left">03.21</td>
<td align="left">Bayesian optimization</td>
<td align="left">[<a href="tut/tut08.pdf">slides</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut08_1.ipynb">notebook 1</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut08_2.ipynb">notebook 2</a>]</td>
</tr>
<tr class="odd">
<td align="left">Tutorial 9</td>
<td align="left">03.28</td>
<td align="left">Reinforcement learning</td>
<td align="left">[<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut09.ipynb">notebook</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 10</td>
<td align="left">04.04</td>
<td align="left">Final review</td>
<td align="left">[<a href="tut/tut10.pdf">slides</a>]</td>
</tr>
</tbody>
</table>
<hr /> -->


<!-- <h2 id="paper-readings"><a name="readings">Paper Readings</a></h2>
<p>5% of your total mark is allocated to reading a set of classic machine learning papers. We hope these papers are both interesting and understandable given what you learn in this course. Please select 2 papers of your interest from the reading list below. You will need to hand in reading notes for the papers you select. The notes should include a summary of the paper's main contribution and your view of the paper's strengths and weaknesses. Submit the notes on MarkUs under file name <code>reading.pdf</code>. A completion mark of 5% will be given.</p>
<ul>
<li><p>Viola, Paul, and Michael Jones. &quot;Rapid object detection using a boosted cascade of simple features.&quot; Computer Vision and Pattern Recognition, 2001. [<a href="http://www.robots.ox.ac.uk/~cvrg/trinity2002/CVPR-2001.pdf">pdf</a>]</p></li>
<li><p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &quot;Imagenet classification with deep convolutional neural networks.&quot; Advances in neural information processing systems. 2012. [<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">pdf</a>]</p></li>
<li><p>Mnih, Andriy, and Ruslan R. Salakhutdinov. &quot;Probabilistic matrix factorization.&quot; Advances in neural information processing systems. 2008. [<a href="http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf">pdf</a>]</p></li>
<li><p>Olshausen, Bruno A., and David J. Field. &quot;Sparse coding with an overcomplete basis set: A strategy employed by V1?.&quot; Vision research 37.23 (1997): 3311-3325. [<a href="http://redwood.psych.cornell.edu/papers/olshausen_field_1997.pdf">pdf</a>]</p></li>
<li><p>Mnih, Volodymyr, et al. &quot;Human-level control through deep reinforcement learning.&quot; Nature 518.7540 (2015): 529. [<a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf">pdf</a>]</p></li>
<li><p>Hardt, Moritz, Eric Price, and Nati Srebro. &quot;Equality of opportunity in supervised learning.&quot; Advances in neural information processing systems. 2016. [<a href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">pdf</a>]</p></li>
<li><p>Tsochantaridis, Ioannis, et al. &quot;Large margin methods for structured and interdependent output variables.&quot; Journal of machine learning research 6.Sep (2005): 1453-1484. [<a href="http://www.jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf">pdf</a>]</p></li>
<li><p>Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. &quot;Sequence to sequence learning with neural networks.&quot; Advances in neural information processing systems. 2014. [<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">pdf</a>]</p></li>
<li><p>Ren, Shaoqing, et al. &quot;Faster r-cnn: Towards real-time object detection with region proposal networks.&quot; Advances in neural information processing systems. 2015. [<a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">pdf</a>]</p></li>
<li><p>Coates, Adam, and Andrew Y. Ng. &quot;The importance of encoding versus training with sparse coding and vector quantization.&quot; Proceedings of the 28th international conference on machine learning. 2011. [<a href="http://www.robotics.stanford.edu/~ang/papers/icml11-EncodingVsTraining.pdf">pdf</a>]</p></li>
<li><p>Kingma, Diederik P., and Max Welling. &quot;Auto-encoding variational bayes.&quot; Proceedings of the 2nd international conference on learning representations. 2014. [<a href="https://arxiv.org/pdf/1312.6114.pdf">pdf</a>]</p></li>
<li><p>Bottou, Léon, and Olivier Bousquet. &quot;The tradeoffs of large scale learning.&quot; Advances in neural information processing systems. 2008. [<a href="http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf">pdf</a>]</p></li>
<li><p>Neal, Radford and Hinton, Geoffrey. “A view of the EM algorithm that justifies incremental, sparse, and other variants.” Learning in graphical models. 1999. [<a href="http://www.cs.toronto.edu/~hinton/absps/emk.pdf">pdf</a>]</p></li>
<li><p>Tipping, Michael and Bishop, Christopher. “Probabilistic principal component analysis.” Journal of the royal statistical society. 1999. [<a href="http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf">pdf</a>]</p></li>
</ul>
<hr />
<h2 id="resources"><a name="resources">Resources</a></h2>
<h3 id="suggested-readings">Suggested Readings</h3>
<ul>
<li>ESL: The Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman. [<a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">link</a>]</li>
<li>MacKay: Information Theory, Inference, and Learning Algorithms, by David MacKay. [<a href="http://www.inference.org.uk/itila/book.html">link</a>]</li>
<li>Barber: Bayesian Reasoning and Machine Learning, by David Barber. [<a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf">link</a>]</li>
<li>Bishop: Pattern Recognition and Machine Learning, by Chris Bishop. [<a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">link</a>]</li>
<li>Sutton and Barto: Reinforcement Learning: An Introduction, by Sutton and Barto. [<a href="http://incompleteideas.net/book/the-book-2nd.html">link</a>]</li>
</ul>
 -->



<h2 id="project"><a name="project">Final Project</a></h2>

<p>The final project is an individual project. For requirements of the
final project, please see the <a href="bayesian_project.pdf">final
project guidelines</a>. The LaTeX template for the project report is
<a href="template.zip">here</a>. </p>

<h2 id="acknowledgements"><a name="acknowledgement">Acknowledgements</a></h2> 
The course materials are adapted from the related courses offered by David Blei, Yang Chen, Andrew Gelman, Long Nguyen, and Scott Linderman.


<div class="ribbon">

</div>



</section>

<!-- </div> -->
<!-- Sticky -->
<!-- </div> -->
<!-- .right-col-block -->
<!-- </div> -->
<div class="row" style="padding-bottom: 25%"> </div>
<!-- .columns-block -->

<!-- #main-wrapper -->

<!-- jquery -->
<script src="../../../js/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="../../../js/bootstrap.min.js"></script>
<script src="../../../js/theia-sticky-sidebar.js"></script>
<script src="../../../js/scripts.js"></script>
</body>
</html>