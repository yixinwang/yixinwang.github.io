<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>Bayesian Data Analysis</title>
    <meta name="description" content="Bayesian Data Analysis">
    <meta name="author" content="Yixin Wang">

    <!-- Favicon
    –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="icon" href="images/favicon.ico" type="image/x-icon" />

    <!-- FONT
    –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

    <!-- font-awesome -->
    <link href="../../../css/font-awesome.min.css" rel="stylesheet">

    <!-- Bootstrap -->
    <link href="../../../css/bootstrap.min.css" rel="stylesheet">

    <!-- Style CSS -->
    <link href="../../../css/style.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/normalize.css">
    <link rel="stylesheet" href="../../../css/skeleton.css">
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
</head>
<body id="page-top" data-spy="scroll" data-target=".navbar">
<div id="main-wrapper">
<!-- Page Preloader -->
<div id="preloader">
    <div id="status">
        <div class="status-mes"></div>
    </div>
</div>

<div class="columns-block container">



<!-- <div class="four fixedcolumns"> -->
    <!-- <header class="header theiaStickySidebar"> -->


<!-- <div class="nine columns"> -->
<!-- <div class="theiaStickySidebar"> -->
<section class="expertise-wrapper section-wrapper gray-bg">
    <!-- <div class="container-fluid"> -->
        <!-- <div class="row"> -->

<div class="ribbon">

</div>
<div>
<h1>
STATS/DATASCI 451 Fall 2023
</h1>
<h1>
Bayesian Data Analysis
</h1>
</div>
<hr />

<div class="nav-bar">
<p><a href="#overview">Overview</a> | <a href="#calendar">Course Calendar</a> | <a href="#lectures">Lecture Schedule</a> 
</div>
<hr />

<h2 id="overview"><a name="overview">Overview</a></h2>


<p>The course is an introduction to both principles and practice of
Bayesian inference for data analysis. We will focus on building
probabilistic models, algorithms for approximate Bayesian inference,
and methods for checking, criticizing, and revising models. Some of
the models we will study include classic Bayesian mixture and
regression models, hierarchical models, factor models, topic models,
and deep generative models. Alongside these models we will study
algorithms for approximate Bayesian inference including Markov Chain
Monte Carlo and variational inference algorithms. Finally, we will
discuss methods for checking, criticizing, and revising models in an
iterative manner, completing a virtuous cycle of applied Bayesian
statistics.</p>

<p>At the end of this course students will be familiar with the
Bayesian paradigm, and will be able to analyze different classes of
statistical models. The course gives an introduction to the
computational tools needed for Bayesian data analysis and develops
statistical modeling skills through a hands-on data analysis
approach.</p>

<h3 id="syllabus">Syllabus</h3>

<p>For course policies, course requirements, and grading policies,
please see the syllabus [<a
href="bayesian_syllabus_2023.pdf">link</a>].</p>

<!-- <p>The main requirements of the course are just-in-time teaching (JiTT)
questions (5%), weekly quizzes (24%), weekly homework assignments
(55%), and one final project (16%). Only your top 8 quiz scores and
your top 10 homework scores are counted.</p> -->

<h3 id="piazza">Piazza</h3>
<p>Students should sign up Piazza [<a href="https://piazza.com/umich/fall2023/statsdatasci451">link</a>] to join course discussions.</p>

<p><b>All communications with the teaching team (the instructor and
the GSIs) should be conducted over Piazza; please do not email. </b>
If you'd like to reach the instructor or the GSIs for private
questions, please post a private note on Piazza that is only visible
to the instructor and the GSIs. See <a
href="https://support.piazza.com/support/solutions/articles/48000616669-post-a-private-note">here</a>
for detailed instructions.  The GSIs and the instructor will be
monitoring piazza, endorsing correct student answers, and answering
questions that remain after a discussion.</p>

<p><b>As a bonus, up to 3 percentage points will be added to your
final course grade based on piazza participation.</b> You will get
(3x/100) bonus percentage points if the number of your total Piazza
contributions is (x * 100)% of the maximum number of contributions
among all students. The number of Piazza contributions will be
determined by Piazza class statistics.</p>

<h3 id="teaching-staff">Teaching Team and Office Hours</h3>
<ul>
<li>Instructor: Yixin Wang </li>
<ul>
<li>Office Hour: Weekly <a
href="https://calendar.google.com/calendar/u/1?cid=Y19vcGlqcWhrZjloM2MzZHBhb2dkcGh1aHU3OEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t">Office
Hours</a>. </li>
</ul></li>

<li>GSI: 
<ul>
<li>Zhiwei Xu (Office Hour: Monday 4:30-5:30pm virtual, Wednesday 2:20-3:50pm in person, Friday 9:00-9:30am virtual)</li> 
<li>Akhil Goel (Office Hour: Tuesdays: 4:30-6:00pm in person, Thursdays: 9-10:30am virtual)</li> 
</ul>
In person GSI Office Hours: Angel Hall G219 </br>
Virtual GSI Office Hours: <a href="https://umich.zoom.us/j/8636631834">https://umich.zoom.us/j/8636631834</a></br>
Please refer to course calendar for details.
</li>

<!-- <h3 id="when-where">When and Where</h3> -->


<h2 id="calendar"><a name="calendar">Course Calendar</a></h2>
<ul>
<li>Lecture: Tue/Thur 11:30am-12:50pm </li>
<li>Location: Auditorium D Angel Hall </li>
<li>Google Calendar: The Google Calendar below ideally contains all events and deadlines for student's convenience. Please feel free to add this calendar to your Google Calendar by clicking on the plus (+) button on the bottom right corner of the calendar below. Any adhoc changes to the schedule will be visible on the calendar first. </li>

<iframe src="https://calendar.google.com/calendar/embed?height=600&wkst=1&bgcolor=%23ffffff&ctz=America%2FNew_York&showTitle=1&showNav=1&showTabs=1&mode=AGENDA&src=Y2M2MjI2aHRxODhhNzNpZnE3dWtnam84MGtAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&color=%234285F4" style="border-width:0" width="800" height="400" frameborder="0" scrolling="no"></iframe></li>

</ul>

<h2 id="lectures"><a name="lectures">Lecture Schedule</a></h2>

<p>The Schedule is subject to change.</p>

<p>By each date, please read about
the topic at hand; please choose one reading from the list for the
topic.</p>

Kruschke = Doing Bayesian Data Analysis [<a href="https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf">link</a>]</br>
BDA = Bayesian Data Analysis by Gelman [<a href="http://www.stat.columbia.edu/~gelman/book/">link</a>]</br>
PML = Probabilistic Machine Learning: Advanced Topics by Murphy [<a href="https://probml.github.io/pml-book/book2.html">link</a>]</br>
PRML = Pattern Recognition and Machine Learning by Bishop [<a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">link</a>]</br>
SR = Statistical Rethinking by McElreath [<a href="https://civil.colorado.edu/~balajir/CVEN6833/bayes-resources/RM-StatRethink-Bayes.pdf">link</a>]</br>
<p></p>

<table>
<colgroup>
<col width="15%" />
<col width="15%" />
<col width="40%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Date</th>
<th align="left">Topic</th>
<th align="left">Readings</th>
<td align="left"><p></p></td>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><p>Lecture 1</p></td>
<td align="left"><p>08/29</p></td>
<td align="left"><p>Introduction I</p></td>
<td align="left">
<p> Kruschke ch. 2</br>
<p>"Bayesian data analysis for newcomers" <a href="https://link.springer.com/article/10.3758/s13423-017-1272-1">(Kruschke and Liddel, 2018)</a></br>
<p>"R Basics with Google Colab" <a href="https://colab.research.google.com/drive/1iz6ILnVGt8Qc6UR1l7oTPou4l6WSrw9S?usp=sharing">Notebook</a> <a href="https://www.youtube.com/watch?v=qE_nQPojhhw">Video</a></p></td>
<td align="left"><p></p></td>
</tr>

<tr class="even">
<td align="left"><p>Lecture 2</p></td>
<td align="left"><p>08/31</p></td>
<td align="left"><p>Probability: A Review of Basic Concepts and Bayes’ Theorem I</p></td>
<td align="left">
<p> Kruschke ch. 4</br>
<p><a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/probability_review.pdf">"Review of Probability" (Blei, 2016)</a></br>
<td align="left"><p></p></td>
</tr>



<tr class="odd">
<td align="left"><p>Lecture 3</p></td>
<td align="left"><p>09/05</p></td>
<td align="left"><p>Probability: A Review of Basic Concepts and Bayes’ Theorem II</p></td>
<td align="left">
''
<!-- <p>"The Basics of Graphical Models" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/graphical-models.pdf">(Blei, 2016)</a></br>
"Statistical Concepts" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/models.pdf">(Blei, 2016)</a></br>
BDA, Sec. 2.1-3</br>
SR, Sec. 2.3-5 and Chap. 3</br>
PRML, Sec. 1.2-3</br>
"Model-based Machine Learning"  <a href="http://www.cs.columbia.edu/~blei/fogm/2022F/readings/Bishop2013.pdf">(Bishop, 2013)</a></p></td> -->
<td align="left"><p></p></td>
<!-- ESL 9.2</p></td> -->
</tr>


<tr class="even">
<td align="left"><p>Lecture 4</p></td>
<td align="left"><p>09/07</p></td>
<td align="left"><p>The Basics of Bayesian Statistics I</p></td>
<td align="left">
<p> Kruschke ch. 5</br>
<!-- <p>Sec. 4 of "The Exponential Family" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/exponential_families.pdf">(Blei, 2016)</a></br>
PML, Sec 3.1-5</br>
BDA, Sec. 2.4-9</p></td> -->
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 5</p></td>
<td align="left"><p>09/12</p></td>
<td align="left"><p>The Basics of Bayesian Statistics II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 6</p></td>
<td align="left"><p>09/14</p></td>
<td align="left"><p>The Basics of Bayesian Statistics III</p></td>
<td align="left"><p>''</td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 7</p></td>
<td align="left"><p>09/19</p></td>
<td align="left"><p>The Beta-Binomial Model and the Bayesian Workflow I</p></td>
<td align="left">
<p> Kruschke ch. 6</br>
<p>"Posterior Predictive Checks" <a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/ppc.pdf">(Blei, 2011)</a></br>
<!-- BDA, Sec. 6.1-5</br>
PML, Sec. 3.9</p></td> -->
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 8</p></td>
<td align="left"><p>09/21</p></td>
<td align="left"><p>The Beta-Binomial Model and the Bayesian Workflow II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 9</p></td>
<td align="left"><p>09/26</p></td>
<td align="left"><p>The Exchangeable Data Model and Conjugate Priors I</p></td>
<td align="left"><p>BDA, Sec. 2.4-9</a></br>
</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 10</p></td>
<td align="left"><p>09/28</p></td>
<td align="left"><p>The Exchangeable Data Model and Conjugate Priors II</p></td>
<td align="left"><p>''</a></br>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 11</p></td>
<td align="left"><p>10/03</p></td>
<td align="left"><p>The Exchangeable Data Model and Conjugate Priors III</p></td>
<td align="left"><p>''</br>
</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 12</p></td>
<td align="left"><p>10/05</p></td>
<td align="left"><p>The Exchangeable Data Model and Conjugate Priors IV</br>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 13</p></td>
<td align="left"><p>10/10</p></td>
<td align="left"><p>Bayesian Computation and an Introduction to Stan I</p></td>
<td align="left"><p>Kruschke ch. 7, 14</br>
<!-- <p>Sec. 1-2 of "Linear regression, Logistic regression, and Generalized Linear Models"<a href="http://www.cs.columbia.edu/~blei/fogm/2014F/lectures/glms.pdf"> (Blei, 2014)</a></br>
BDA, Chap. 14</br>
PML, Sec. 15.1-2</br>
SR, Chap. 4</br></p></td> -->
</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 14</p></td>
<td align="left"><p>10/12</p></td>
<td align="left"><p>Bayesian Computation and an Introduction to Stan II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Fall break</p></td>
<td align="left"><p>10/17</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 15</p></td>
<td align="left"><p>10/19</p></td>
<td align="left"><p>Midterm Exam</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="odd">
<td align="left"><p>Lecture 16</p></td>
<td align="left"><p>10/24</p></td>
<td align="left"><p>Grouped Data and Hierarchical Models I</p></td>
<td align="left"><p> Kruschke ch. 9</br>
<p>Kruschke ch. 12.1, 16</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 17</p></td>
<td align="left"><p>10/26</p></td>
<td align="left">Grouped Data and Hierarchical Models II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 18</p></td>
<td align="left"><p>10/31</p></td>
<td align="left"><p><p>Conditional Models: Linear and Logistic Regression I</p></td>
<td align="left">
<p>Kruschke ch. 15, 17
<!-- <p>"Bayesian Mixture Models and the Gibbs Sampler"<a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/gibbs.pdf"> (Blei, 2016)</a></br>
BDA, Chap. 22</br>
PML, Sec. 12.1-3</br>
SR, Chap. 9</br>
PRML, Chap. 11</br>
"Identifying Bayesian Mixture Models" <a href="https://mc-stan.org/users/documentation/case-studies/identifying_mixture_models.html">(Betancourt, 2018)</a> -->
</br></p></td>
<td align="left"><p>
</p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 19</p></td>
<td align="left"><p>11/02</p></td>
<td align="left"><p>Conditional Models: Linear and Logistic Regression II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 20</p></td>
<td align="left"><p>11/07</p></td>
<td align="left"><p>Conditional Models: Linear and Logistic Regression III</p></td>
<td align="left"><p>''
<!--     "Mixed-membership Models (and an Introduction to Variational Inference)" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/mixed_membership.pdf">(Blei, 2016)</a></br>
[Video Tutorial] Variational Inference: Foundations and Innovations (Blei, 2019) <a href="https://www.youtube.com/watch?v=DaqNNLidswA">(Part 1) </a> [<a href="http://www.cs.columbia.edu/~blei/talks/Blei_VI_tutorial.pdf">slides</a>]</br>
PML, Sec. 28.5<br />
PRML, Sec. 10.1-2</br>
BDA, Sec. 13.7<br />
"Probabilistic Topic Models" <a href="http://www.cs.columbia.edu/~blei/fogm/2020F/readings/Blei2012.pdf">(Blei, 2012)</a></br>
"Variational Inference: A Review for Statisticians” <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773">(Blei et al, 2017)</a> -->
</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="odd">
<td align="left"><p>Lecture 21</p></td>
<td align="left"><p>11/09</p></td>
<td align="left"><p>Guest Lecture: Advanced Bayesian Ideas in Artificial Intelligence</p></td>
<td align="left"><p></p></td>
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 22</p></td>
<td align="left"><p>11/14</p></td>
<td align="left"><p>Introduction to Markov Chain Monte Carlo I</p></td>
<td align="left">
<p>BDA, Chap. 22</br>
<p>"Bayesian Mixture Models and the Gibbs Sampler"<a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/gibbs.pdf"> (Blei, 2016)</a></br>
<p>Kruschke ch. 7, 14</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="odd">
<td align="left"><p>Lecture 23</p></td>
<td align="left"><p>11/16</p></td>
<td align="left"><p>Introduction to Markov Chain Monte Carlo II</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="even">
<td align="left"><p>Lecture 24</p></td>
<td align="left"><p>11/21</p></td>
<td align="left"><p>Introduction to Markov Chain Monte Carlo III</p></td>
<td align="left"><p>''
<!--     "Matrix Factorization" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/matrix_factorization.pdf"> (Blei, 2016)</a></br>
PML, Sec. 28.3-4</br>
PRML, Sec. 12.2-4</br>
"Scalable Recommendation with Hierarchical Poisson Factorization" <a href="http://www.cs.columbia.edu/~blei/fogm/2022F/readings/GopalanHofmanBlei2015.pdf">(Gopalan et al., 2015)</a></br> -->
</p></td>
<td align="left"><p></p></td>
</tr>



<tr class="odd">
<td align="left"><p>Thanksgiving break</p></td>
<td align="left"><p>11/23</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 25</p></td>
<td align="left"><p>11/28</p></td>
<td align="left"><p>Introduction to Markov Chain Monte Carlo IV</p></td>
<td align="left">
<!-- <p>"Mixed-membership Models (and an Introduction to Variational Inference)" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/mixed_membership.pdf">(Blei, 2016)</a></br>
[Video Tutorial] Variational Inference: Foundations and Innovations (Blei, 2019) <a href="https://www.youtube.com/watch?v=DaqNNLidswA">(Part 1) </a> [<a href="http://www.cs.columbia.edu/~blei/talks/Blei_VI_tutorial.pdf">slides</a>]</br>
PML, Sec. 28.5<br />
PRML, Sec. 10.1-2</br>
BDA, Sec. 13.7<br />
"Probabilistic Topic Models" <a href="http://www.cs.columbia.edu/~blei/fogm/2020F/readings/Blei2012.pdf">(Blei, 2012)</a></br>
"Variational Inference: A Review for Statisticians” <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773">(Blei et al, 2017)</a> -->
<!-- Sec. 1-3 of "The Exponential Family" <a href="http://www.cs.columbia.edu/~blei/fogm/2016F/doc/exponential_families.pdf">(Blei, 2016)</a></br>
Sec. 3 of "Linear regression, Logistic regression, and Generalized Linear Models"<a href="http://www.cs.columbia.edu/~blei/fogm/2014F/lectures/glms.pdf"> (Blei, 2014)</a></br>
BDA, Chap. 16</br>
PRML, Sec. 2.4</br>
SR, Chap. 9-10</br>
PML, Sec. 2.3, 3.4, 10.3.1</a> -->
''</p></td>
<td align="left"><p></p></td>
</tr>


<tr class="even">
<td align="left"><p>Lecture 26</p></td>
<td align="left"><p>11/30</p></td>
<td align="left"><p>An Introduction to Variational Inference</p></td>
<td align="left"><p>[Video Tutorial] Variational Inference: Foundations and Innovations (Blei, 2019) <a href="https://www.youtube.com/watch?v=DaqNNLidswA">(Part 1) </a> [<a href="http://www.cs.columbia.edu/~blei/talks/Blei_VI_tutorial.pdf">slides</a>]</br></a></p></td>
<td align="left"><p></p></td>
</tr>
<!-- <tr class="even"> -->
<!-- <td align="left"><p>Lecture 25</p></td>
<td align="left"><p>11/29</p></td>
<td align="left"><p>Deep Generative Models and Black Box Variational Inference I</p></td>
<td align="left"><p>"Variational Autoencoders"<a href="https://deepgenerativemodels.github.io/notes/vae/"> (Grover, 2018)</a></br>
[Video Tutorial] "Variational Inference: Foundations and Innovations" (Blei, 2019) <a href="https://www.youtube.com/watch?v=Wd7R_YX4PcQ">(Part 2) </a> [<a href="http://www.cs.columbia.edu/~blei/talks/Blei_VI_tutorial.pdf">slides</a>]</br>
"An Introduction to Variational Autoencoders" Chap. 1-2 <a href="https://arxiv.org/pdf/1906.02691.pdf">(Kingma and Welling, 2019)</a></br>
PML, Sec. 10.3, 21.2</br>
</p></td>
<td align="left"><p></p></td>
</tr>
 -->

<tr class="odd">
<td align="left"><p>Lecture 27</p></td>
<td align="left"><p>12/05</p></td>
<td align="left"><p>Summary (and wiggle room)</p></td>
<td align="left"><p>''</p></td>
<td align="left"><p></p></td>
</tr>

<!-- 
<tr class="even">
<td align="left"><p>Lecture 28</p></td>
<td align="left"><p>12/07</p></td>
<td align="left"><p>Summary (and wiggle room)</p></td>
<td align="left"><p>------------</p></td>
<td align="left"><p></p></td>
</tr> -->
</tbody>
</table>
<hr />


<!-- 

<h2 id="project"><a name="project">Final Project</a></h2>

<p>The final project is an individual project. For requirements of the
final project, please see the <a href="bayesian_project.pdf">final
project guidelines</a>. The LaTeX template for the project report is
<a href="template.zip">here</a>. </p> -->

<!-- <h2 id="tutorials"><a name="tutorials">Tutorials</a></h2>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Date</th>
<th align="left">Topic</th>
<th align="left">Materials</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Tutorial 1</td>
<td align="left">01.10</td>
<td align="left">Probability review</td>
<td align="left">[<a href="tut/tut01.pdf">slides</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 2</td>
<td align="left">01.17</td>
<td align="left">Linear algebra review</td>
<td align="left">[<a href="tut/tut02.pdf">slides</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut02.ipynb">demo</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut02_worksheet.ipynb">exercise</a>]</td>
</tr>
<tr class="odd">
<td align="left">Tutorial 3</td>
<td align="left">01.24</td>
<td align="left">Gradient descent</td>
<td align="left">[<a href="tut/tut03.pdf">slides</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut03.ipynb">demo</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut03_worksheet.ipynb">exercise</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 4</td>
<td align="left">01.31</td>
<td align="left">Linear algebra review</td>
<td align="left">[<a href="tut/tut04.pdf">slides</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut04.ipynb">demo</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut04_worksheet.ipynb">exercise</a>]</td>
</tr>
<tr class="odd">
<td align="left">Tutorial 5</td>
<td align="left">02.07</td>
<td align="left">Midterm review</td>
<td align="left">[<a href="tut/tut05.pdf">slides</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 6</td>
<td align="left">02.28</td>
<td align="left">MCMC</td>
<td align="left">[<a href="tut/tut06.pdf">slides</a>]</td>
</tr>
<tr class="odd">
<td align="left">Tutorial 7</td>
<td align="left">03.14</td>
<td align="left">Multivariate Gaussian</td>
<td align="left">[<a href="tut/tut07.pptx">slides</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 8</td>
<td align="left">03.21</td>
<td align="left">Bayesian optimization</td>
<td align="left">[<a href="tut/tut08.pdf">slides</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut08_1.ipynb">notebook 1</a>] [<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut08_2.ipynb">notebook 2</a>]</td>
</tr>
<tr class="odd">
<td align="left">Tutorial 9</td>
<td align="left">03.28</td>
<td align="left">Reinforcement learning</td>
<td align="left">[<a href="https://nbviewer.jupyter.org/url/www.cs.toronto.edu/~mren/teach/csc411_19s/tut/tut09.ipynb">notebook</a>]</td>
</tr>
<tr class="even">
<td align="left">Tutorial 10</td>
<td align="left">04.04</td>
<td align="left">Final review</td>
<td align="left">[<a href="tut/tut10.pdf">slides</a>]</td>
</tr>
</tbody>
</table>
<hr /> -->


<!-- <h2 id="paper-readings"><a name="readings">Paper Readings</a></h2>
<p>5% of your total mark is allocated to reading a set of classic machine learning papers. We hope these papers are both interesting and understandable given what you learn in this course. Please select 2 papers of your interest from the reading list below. You will need to hand in reading notes for the papers you select. The notes should include a summary of the paper's main contribution and your view of the paper's strengths and weaknesses. Submit the notes on MarkUs under file name <code>reading.pdf</code>. A completion mark of 5% will be given.</p>
<ul>
<li><p>Viola, Paul, and Michael Jones. &quot;Rapid object detection using a boosted cascade of simple features.&quot; Computer Vision and Pattern Recognition, 2001. [<a href="http://www.robots.ox.ac.uk/~cvrg/trinity2002/CVPR-2001.pdf">pdf</a>]</p></li>
<li><p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. &quot;Imagenet classification with deep convolutional neural networks.&quot; Advances in neural information processing systems. 2012. [<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">pdf</a>]</p></li>
<li><p>Mnih, Andriy, and Ruslan R. Salakhutdinov. &quot;Probabilistic matrix factorization.&quot; Advances in neural information processing systems. 2008. [<a href="http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf">pdf</a>]</p></li>
<li><p>Olshausen, Bruno A., and David J. Field. &quot;Sparse coding with an overcomplete basis set: A strategy employed by V1?.&quot; Vision research 37.23 (1997): 3311-3325. [<a href="http://redwood.psych.cornell.edu/papers/olshausen_field_1997.pdf">pdf</a>]</p></li>
<li><p>Mnih, Volodymyr, et al. &quot;Human-level control through deep reinforcement learning.&quot; Nature 518.7540 (2015): 529. [<a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf">pdf</a>]</p></li>
<li><p>Hardt, Moritz, Eric Price, and Nati Srebro. &quot;Equality of opportunity in supervised learning.&quot; Advances in neural information processing systems. 2016. [<a href="http://papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf">pdf</a>]</p></li>
<li><p>Tsochantaridis, Ioannis, et al. &quot;Large margin methods for structured and interdependent output variables.&quot; Journal of machine learning research 6.Sep (2005): 1453-1484. [<a href="http://www.jmlr.org/papers/volume6/tsochantaridis05a/tsochantaridis05a.pdf">pdf</a>]</p></li>
<li><p>Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. &quot;Sequence to sequence learning with neural networks.&quot; Advances in neural information processing systems. 2014. [<a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">pdf</a>]</p></li>
<li><p>Ren, Shaoqing, et al. &quot;Faster r-cnn: Towards real-time object detection with region proposal networks.&quot; Advances in neural information processing systems. 2015. [<a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">pdf</a>]</p></li>
<li><p>Coates, Adam, and Andrew Y. Ng. &quot;The importance of encoding versus training with sparse coding and vector quantization.&quot; Proceedings of the 28th international conference on machine learning. 2011. [<a href="http://www.robotics.stanford.edu/~ang/papers/icml11-EncodingVsTraining.pdf">pdf</a>]</p></li>
<li><p>Kingma, Diederik P., and Max Welling. &quot;Auto-encoding variational bayes.&quot; Proceedings of the 2nd international conference on learning representations. 2014. [<a href="https://arxiv.org/pdf/1312.6114.pdf">pdf</a>]</p></li>
<li><p>Bottou, Léon, and Olivier Bousquet. &quot;The tradeoffs of large scale learning.&quot; Advances in neural information processing systems. 2008. [<a href="http://papers.nips.cc/paper/3323-the-tradeoffs-of-large-scale-learning.pdf">pdf</a>]</p></li>
<li><p>Neal, Radford and Hinton, Geoffrey. “A view of the EM algorithm that justifies incremental, sparse, and other variants.” Learning in graphical models. 1999. [<a href="http://www.cs.toronto.edu/~hinton/absps/emk.pdf">pdf</a>]</p></li>
<li><p>Tipping, Michael and Bishop, Christopher. “Probabilistic principal component analysis.” Journal of the royal statistical society. 1999. [<a href="http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf">pdf</a>]</p></li>
</ul>
<hr />
<h2 id="resources"><a name="resources">Resources</a></h2>
<h3 id="suggested-readings">Suggested Readings</h3>
<ul>
<li>ESL: The Elements of Statistical Learning, by Hastie, Tibshirani, and Friedman. [<a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">link</a>]</li>
<li>MacKay: Information Theory, Inference, and Learning Algorithms, by David MacKay. [<a href="http://www.inference.org.uk/itila/book.html">link</a>]</li>
<li>Barber: Bayesian Reasoning and Machine Learning, by David Barber. [<a href="http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf">link</a>]</li>
<li>Bishop: Pattern Recognition and Machine Learning, by Chris Bishop. [<a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">link</a>]</li>
<li>Sutton and Barto: Reinforcement Learning: An Introduction, by Sutton and Barto. [<a href="http://incompleteideas.net/book/the-book-2nd.html">link</a>]</li>
</ul>
 -->


<h2 id="acknowledgements"><a name="acknowledgement">Acknowledgements</a></h2> 
The course materials are adapted from the related courses offered by David Blei, Yang Chen, Andrew Gelman, and Scott Linderman.


<div class="ribbon">

</div>



</section>

<!-- </div> -->
<!-- Sticky -->
<!-- </div> -->
<!-- .right-col-block -->
<!-- </div> -->
<div class="row" style="padding-bottom: 25%"> </div>
<!-- .columns-block -->

<!-- #main-wrapper -->

<!-- jquery -->
<script src="../../../js/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="../../../js/bootstrap.min.js"></script>
<script src="../../../js/theia-sticky-sidebar.js"></script>
<script src="../../../js/scripts.js"></script>
</body>
</html>